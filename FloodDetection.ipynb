{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import regularizers, optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\nfrom collections import Counter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-13T10:01:34.863915Z","iopub.execute_input":"2022-02-13T10:01:34.864891Z","iopub.status.idle":"2022-02-13T10:01:41.512918Z","shell.execute_reply.started":"2022-02-13T10:01:34.864779Z","shell.execute_reply":"2022-02-13T10:01:41.512046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_label = pd.read_csv('../input/fptu-huynhld3-spr22/dev/devset_images_gt.csv', names=['id', 'label'])\nimage_label['id'] = image_label['id'].apply(lambda x: '{}.jpg'.format(x))\nimage_label['label'] = image_label['label'].astype(str)\nimage_label = image_label.sample(frac=1, random_state=3).reset_index(drop=True)\ntrain_data = image_label.iloc[:int(image_label.shape[0]*0.8)].copy(deep=True)\nvalid_data = image_label.iloc[int(image_label.shape[0]*0.8):].copy(deep=True)\n# image_label = list(image_label.to_records(index=False))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:01:41.514917Z","iopub.execute_input":"2022-02-13T10:01:41.515328Z","iopub.status.idle":"2022-02-13T10:01:41.574578Z","shell.execute_reply.started":"2022-02-13T10:01:41.515253Z","shell.execute_reply":"2022-02-13T10:01:41.571294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_gen = image.ImageDataGenerator(rescale=1./255,\n                                       rotation_range=30, width_shift_range=0.3, \n                                       height_shift_range=0.1, \n                                       zoom_range=[0.1, 1], \n                                       horizontal_flip=True,\n                                       brightness_range=[0.2, 1])\ntest_gen = image.ImageDataGenerator(rescale=1./255)\ntrain_data = dataset_gen.flow_from_dataframe(train_data, directory = '../input/fptu-huynhld3-spr22/dev/images', \n                                            x_col = 'id', y_col = 'label', class_mode='categorical', \n                                            target_size=(288, 288), batch_size=64, shuffle=True, seed=2)\nvalid_data = test_gen.flow_from_dataframe(valid_data, directory = '../input/fptu-huynhld3-spr22/dev/images', \n                                            x_col = 'id', y_col = 'label', class_mode='categorical', \n                                            target_size=(288, 288), batch_size=64, shuffle=False, seed=2)\ncounter = Counter(train_data.classes)\n\nprint(counter.items())\ncounter = Counter(valid_data.classes)\n\nprint(counter.items())","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:01:41.580866Z","iopub.execute_input":"2022-02-13T10:01:41.581485Z","iopub.status.idle":"2022-02-13T10:01:52.574157Z","shell.execute_reply.started":"2022-02-13T10:01:41.581358Z","shell.execute_reply":"2022-02-13T10:01:52.573124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --quiet vit-keras\n\nfrom vit_keras import vit\nvit_model = vit.vit_b32(\n        image_size = 288,\n        activation = 'softmax',\n        pretrained = True,\n        include_top = False,\n        pretrained_top = False,\n        classes = 2)\nmodel = Sequential([\n        vit_model,\n        Flatten(),\n        Dropout(0.25),\n        BatchNormalization(),\n        Dense(1024, activation = tfa.activations.gelu),\n        Dropout(0.25),\n        BatchNormalization(),\n        Dense(64, activation = tfa.activations.gelu),\n        Dropout(0.25),\n        BatchNormalization(),\n        Dense(2, 'softmax')],\n        name = 'vision_transformer')\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:01:52.576697Z","iopub.execute_input":"2022-02-13T10:01:52.577289Z","iopub.status.idle":"2022-02-13T10:02:24.9675Z","shell.execute_reply.started":"2022-02-13T10:01:52.577231Z","shell.execute_reply":"2022-02-13T10:02:24.966431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 5e-5\nEPOCHS = 200\n\noptimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\nauc = tf.keras.metrics.AUC()\ncheckpoint_path = './dlp301_model_vit.h5'\n\nmodel.compile(optimizer = optimizer, \n              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n              metrics = ['accuracy', auc])\nkeras_callbacks   = [\n      EarlyStopping(monitor='val_loss', patience=10, mode='min', min_delta=0.0001),\n      ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min')\n]\n\nSTEP_SIZE_TRAIN=train_data.n//train_data.batch_size\nSTEP_SIZE_VALID=valid_data.n//valid_data.batch_size\nmodel.fit(x = train_data,\n          steps_per_epoch = STEP_SIZE_TRAIN,\n          validation_data = valid_data,\n          validation_steps = STEP_SIZE_VALID,\n          callbacks = keras_callbacks,\n          epochs = EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:02:24.971161Z","iopub.execute_input":"2022-02-13T10:02:24.971439Z","iopub.status.idle":"2022-02-13T11:42:13.403922Z","shell.execute_reply.started":"2022-02-13T10:02:24.971372Z","shell.execute_reply":"2022-02-13T11:42:13.401795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = model.evaluate(valid_data)\nresult","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:47:16.003195Z","iopub.status.idle":"2022-02-02T13:47:16.003876Z","shell.execute_reply.started":"2022-02-02T13:47:16.003628Z","shell.execute_reply":"2022-02-02T13:47:16.003654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\n\nmodel.save('./dlp301_model_vit_ver3.h5')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:47:16.00513Z","iopub.status.idle":"2022-02-02T13:47:16.005818Z","shell.execute_reply.started":"2022-02-02T13:47:16.005555Z","shell.execute_reply":"2022-02-02T13:47:16.00558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\n\npreds = model.predict(valid_data)\nyhat = preds[:, 1]\n# calculate roc curves\nfpr, tpr, thresholds = roc_curve(valid_data.classes, yhat)\nprint(roc_auc_score(valid_data.classes, yhat))\n\npyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\npyplot.plot(fpr, tpr, marker='.', label='Model')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\n# show the plot\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T11:45:40.494518Z","iopub.execute_input":"2022-02-13T11:45:40.495965Z","iopub.status.idle":"2022-02-13T11:45:57.865984Z","shell.execute_reply.started":"2022-02-13T11:45:40.495919Z","shell.execute_reply":"2022-02-13T11:45:57.864718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import sqrt, argmax\ngmeans = sqrt(tpr * (1 - fpr))\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T11:46:07.096271Z","iopub.execute_input":"2022-02-13T11:46:07.096588Z","iopub.status.idle":"2022-02-13T11:46:07.103507Z","shell.execute_reply.started":"2022-02-13T11:46:07.096557Z","shell.execute_reply":"2022-02-13T11:46:07.102197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = np.arange(len(tpr))\nroc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(thresholds, index = i)})\nroc.iloc[(roc.tf-0).abs().argsort()[:1]]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:47:16.010846Z","iopub.status.idle":"2022-02-02T13:47:16.011493Z","shell.execute_reply.started":"2022-02-02T13:47:16.011248Z","shell.execute_reply":"2022-02-02T13:47:16.011273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generator = test_gen.flow_from_directory(directory='../input/fptu-huynhld3-spr22/', classes=['test'], class_mode=None, shuffle=False, target_size=(288, 288), batch_size=64)\npreds = model.predict(test_generator)\nfile_id = []\nfor filepath in test_generator.filenames:\n    file_name = filepath.split('/')[-1]\n    tmp_file_id = file_name.split('.')[0]\n    file_id.append(tmp_file_id)\nfile_id_flood = []\nprob_flood = []\nfor id, pred in zip(file_id, preds):\n    if pred[1]>=0.5:\n        file_id_flood.append(id)\n        prob_flood.append(pred[1])\nsubmission = pd.DataFrame()\nsubmission['id'] = file_id_flood\nsubmission['prediction'] = prob_flood\nsubmission = submission.sort_values(by=['prediction'], ascending=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-02-13T11:46:12.309044Z","iopub.execute_input":"2022-02-13T11:46:12.309324Z","iopub.status.idle":"2022-02-13T11:46:38.586992Z","shell.execute_reply.started":"2022-02-13T11:46:12.309293Z","shell.execute_reply":"2022-02-13T11:46:38.585998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = submission['id'].copy(deep=True)\nsubmission.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T11:48:01.273853Z","iopub.execute_input":"2022-02-13T11:48:01.274175Z","iopub.status.idle":"2022-02-13T11:48:01.286271Z","shell.execute_reply.started":"2022-02-13T11:48:01.274142Z","shell.execute_reply":"2022-02-13T11:48:01.28509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\noptimizer = tfa.optimizers.RectifiedAdam(learning_rate = 5e-5)\nauc = tf.keras.metrics.AUC()\nmodel_2 = load_model('../input/model-weights/dlp301_model_vit_ver2.h5', custom_objects={'auc':auc})\ntest_gen = image.ImageDataGenerator(rescale=1./255)\ntest_generator = test_gen.flow_from_directory(directory='../input/fptu-huynhld3-spr22/', classes=['test'], class_mode=None, shuffle=False, target_size=(224, 224))\npreds = model_2.predict(test_generator)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:47:16.016528Z","iopub.status.idle":"2022-02-02T13:47:16.017152Z","shell.execute_reply.started":"2022-02-02T13:47:16.016915Z","shell.execute_reply":"2022-02-02T13:47:16.016941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_id = []\nfor filepath in test_gen.filenames:\n    file_name = filepath.split('/')[-1]\n    tmp_file_id = file_name.split('.')[0]\n    file_id.append(tmp_file_id)\nfile_id_flood = []\nprob_flood = []\nfor id, pred in zip(file_id, preds):\n    if pred[1]>=0.6:\n        file_id_flood.append(id)\n        prob_flood.append(pred[1])\nsubmission = pd.DataFrame()\nsubmission['id'] = file_id_flood\nsubmission['prediction'] = prob_flood\nsubmission = submission.sort_values(by=['prediction'], ascending=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:32:32.247884Z","iopub.execute_input":"2022-02-13T05:32:32.248223Z","iopub.status.idle":"2022-02-13T05:32:32.282879Z","shell.execute_reply.started":"2022-02-13T05:32:32.248185Z","shell.execute_reply":"2022-02-13T05:32:32.280336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from vit_keras import visualize\n\nx = test_generator.next()\nimage = x[0]\n\nattention_map = visualize.attention_map(model = model, image = image)\n\n# Plot results\nfig, (ax1, ax2) = plt.subplots(ncols = 2)\nax1.axis('off')\nax2.axis('off')\nax1.set_title('Original')\nax2.set_title('Attention Map')\n_ = ax1.imshow(image)\n_ = ax2.imshow(attention_map)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:33:21.271147Z","iopub.execute_input":"2022-02-13T05:33:21.271697Z","iopub.status.idle":"2022-02-13T05:33:22.062882Z","shell.execute_reply.started":"2022-02-13T05:33:21.27166Z","shell.execute_reply":"2022-02-13T05:33:22.061945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:30:58.112756Z","iopub.execute_input":"2022-02-13T05:30:58.11304Z","iopub.status.idle":"2022-02-13T05:30:58.118416Z","shell.execute_reply.started":"2022-02-13T05:30:58.113011Z","shell.execute_reply":"2022-02-13T05:30:58.11768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}